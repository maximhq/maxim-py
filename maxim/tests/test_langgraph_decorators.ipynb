{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf57ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "\n",
    "from langchain_anthropic import  ChatAnthropic\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openAIKey = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "anthropicApiKey = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n",
    "tavilyApiKey = os.environ.get(\"TAVILY_API_KEY\", None)\n",
    "workspaceId = os.environ.get(\"MAXIM_WORKSPACE_ID\", None)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1, tavily_api_key=tavilyApiKey)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def _get_model(model_name: str):\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=openAIKey)\n",
    "    elif model_name == \"anthropic\":\n",
    "        model = ChatAnthropic(\n",
    "            temperature=0,\n",
    "            model_name=\"claude-3-sonnet-20240229\",\n",
    "            api_key=anthropicApiKey,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    model = model.bind_tools(tools)\n",
    "    return model\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Be a helpful assistant\"\"\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get(\"configurable\", {}).get(\"model_name\", \"anthropic\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Define the config\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState, config_schema=GraphConfig)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim import  Maxim\n",
    "from maxim.decorators import current_trace, span, trace\n",
    "from maxim.decorators.langchain import langchain_callback, langgraph_agent\n",
    "\n",
    "maxim_client = Maxim()\n",
    "logger = maxim_client.logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@span(name=\"another-method-span\")\n",
    "def another_method(query: str) -> str:\n",
    "    return query\n",
    "\n",
    "\n",
    "@langgraph_agent(name=\"movie-agent-v1\")\n",
    "async def ask_agent(query: str) -> str:\n",
    "    config = {\"recursion_limit\": 50, \"callbacks\": [langchain_callback()]}\n",
    "    async for event in app.astream(input={\"messages\": [query]}, config=config):\n",
    "        for k, v in event.items():\n",
    "            if k == \"agent\":\n",
    "                response = str(v[\"messages\"][0].content)\n",
    "    return response\n",
    "\n",
    "\n",
    "@trace(logger=logger, name=\"movie-chat-v1\",tags={\"service\": \"movie-chat-v1-server-1\"})\n",
    "async def handle(query: str):\n",
    "    resp = await ask_agent(query)\n",
    "    current_trace().set_output(str(resp))\n",
    "    another_method(str(resp))\n",
    "    trace = current_trace()\n",
    "    trace.feedback({\"score\": 1})\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = await handle(\"is there any new iron man movies coming this year?\")    \n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
